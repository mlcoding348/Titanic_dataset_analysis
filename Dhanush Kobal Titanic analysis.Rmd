---
title: "R Notebook"
output: html_notebook
---

<font size="+3">Introduction</font>

<p>Today we will be analyzing the titanic dataset. We will use various ML models, different cross        validation(CV) tools, and feature selection to see which models gives us the best prediction.
This is a kaggle competition.</p>

```{r, echo = FALSE}
rm(list=ls())
```

```{r, echo=FALSE}
# uploading libraries
library(mlr)
library(tidyverse)
library(ggplot2)
library(parallel)
library(parallelMap)
library(hciR)
library(FSelector)
library(FSelectorRcpp)
library(ltm)
```

```{r, echo=FALSE}
library(readr)
train.kaggle <- read_csv("/Users/dhanushkobal/Desktop/2021\ Projects/Kaggle/titanic/train.csv")
train.kaggle$Type.of.data<-as.factor(rep("Train.data", nrow(train.kaggle)))

test.kaggle <- read_csv("/Users/dhanushkobal/Desktop/2021\ Projects/Kaggle/titanic/test.csv")
test.kaggle$Type.of.data<-as.factor(rep("Test.data", nrow(test.kaggle)))

answer.key<- read_csv("/Users/dhanushkobal/Desktop/2021\ Projects/Kaggle/titanic/Answer.key.csv")
real.values<-answer.key$Survived

train.kaggle<-as_tibble(train.kaggle)
test.kaggle<-as_tibble(test.kaggle)

combined.data<-mutate(bind_rows(train.kaggle , test.kaggle)) # combining train and test data
head(combined.data)
```


<font size="+3">Data preprocessing</font>
<p>Our first step is to modify the varibales to their repsective categories: numeric, categorical, and then we will assess the missing values</p>

```{r, echo=FALSE}
combined.data<- combined.data %>% mutate_at(c("Survived","Pclass" ,"Sex", "Embarked", "Cabin"), as.factor)
```

<p>Now we will assess missing values in our dataset</p>

```{r, echo=FALSE}
cbind(apply(is.na(combined.data),2,sum))
```

<p>These are the missing values we are able to find. ***Age and Cabin*** has the most missing values and ***Fare and Embarked*** has the least missing values</p>

<p>We will try to perform ***EDA*** and see if we can get any insights on how to impute these missing values</p>

<font size="+3">EDA</font>
<font size="+2"><li>EDA Analysis 1: Age</li></font>
```{r, echo=FALSE}
eda.data<-train.kaggle %>% dplyr::select(-c( PassengerId, Type.of.data))
```

***Histogram of age***

```{r, echo=FALSE}
ggplot(data = combined.data, aes(x=Age)) + geom_histogram()
```
<p>We see that age is rouhgly normally distributed</p>

-----
<p>We will now see how ***Age*** correlates with the categogical varibales</p>

```{r, echo=FALSE}
ggplot(data=gather(eda.data, key = "Variable", value = "Value", -c(Age,Fare, Name, Ticket, Cabin)), aes(Value, Age)) +facet_wrap(~ Variable, scale = "free_x") +geom_boxplot()
```
<p>From the looks of this, there seem to be some pattern with Age and Pclass, so we will take a further analysis into this. We are not dealing with ***Parch and SibSp*** just yet since they both seem to be correlated with each other</p>
---

<p>We will explore more with ***Age and Pclass***</p>

```{r, echo=FALSE}
ggplot(data = eda.data , aes(x=factor(Pclass), y=Age)) + geom_boxplot() + facet_wrap(~Sex)
```
<p>From the looks of this, we can impute ***Medain*** values for the age based on Sex and Pclass</p>

```{r, echo=FALSE}
age.missing.value<-rep(1, length(eda.data$Age))
age.missing.value[which(is.na(eda.data$Age))]<-0
eda.data$Age.missing.value<-age.missing.value
eda.data$Age.missing.value<-as.factor(eda.data$Age.missing.value)

age.missing.value<-rep(1, length(combined.data$Age))
age.missing.value[which(is.na(combined.data$Age))]<-0
combined.data$Age.missing.value<-age.missing.value
combined.data$Age.missing.value<-as.factor(combined.data$Age.missing.value)
```


```{r, echo=FALSE}
a<-eda.data %>% group_by(Pclass, Sex) %>% summarise( Age = median(Age, na.rm = TRUE))
b<-full_join(eda.data %>% filter(is.na(Age)), a , by = c('Pclass', 'Sex'))$Age.y
eda.data$Age[which(is.na(eda.data$Age))]<-b


a<-combined.data %>% group_by(Pclass, Sex) %>% summarise( Age = median(Age, na.rm = TRUE))
b<-full_join(combined.data %>% filter(is.na(Age)), a , by = c('Pclass', 'Sex'))$Age.y
combined.data$Age[which(is.na(combined.data$Age))]<-b
```

***Updated histogram for Age***
```{r, echo=FALSE}
ggplot(data = combined.data, aes(x=Age)) + geom_histogram()
```

---
<font size="+2"><li>EDA Analysis 2: Pclass</li></font>

<p>We will first see the realtionship between ***Pclass and Embarked***</p>

```{r, echo=FALSE}
barplot(table(as.factor(eda.data$Pclass), eda.data$Embarked), beside = TRUE, legend.text = TRUE)
eda.data$Pclass.Embarked.relationship<-as.factor(ifelse(eda.data$Pclass==1, "c", "s"))

combined.data$Pclass.Embarked.relationship<-as.factor(ifelse(combined.data$Pclass==1, "c", "s"))
```

<p>From the analysis, we can create a new factor named ***Pclass.Embarked.relationship*** (look at code)</p>



<p>We will take ***SibSp and Parch*** and add them up and call it ***Family.size.numeric ***</p>
```{r, echo=FALSE}
combined.data<-mutate(combined.data, Family.size.numeric = SibSp+Parch+1)
eda.data<-mutate(eda.data, Family.size.numeric = SibSp+Parch+1)
```

```{r, echo=FALSE}
barplot(table(as.factor(eda.data$Pclass), eda.data$Family.size.numeric), beside = TRUE, legend.text = TRUE)

eda.data$famsize.pclass.relationship<-as.factor(ifelse(eda.data$Family.size.numeric %in% c(1,3,5,6,7,8,11) , "Pclass.3",ifelse(eda.data$Family.size.numeric %in% c(4) , "Pclass.2", "Pclass.1")))

combined.data$famsize.pclass.relationship<-as.factor(ifelse(combined.data$Family.size.numeric %in% c(1,3,5,6,7,8,11) , "Pclass.3",ifelse(combined.data$Family.size.numeric %in% c(4) , "Pclass.2", "Pclass.1")))
```

-----
<font size="+2"><li>EDA Analysis 3: Fare</li></font>

```{r}
# Only 1 value has a missing value for Fare
length(which(is.na(combined.data$Fare)))
```

<p>We will impute the missing values of ***Fare*** with the median value</p>

```{r}
combined.data$Fare[which(is.na(combined.data$Fare))]<-median(combined.data$Fare, na.rm = TRUE)
```


```{r, echo=FALSE}
ggplot(data = eda.data, aes(x = Fare)) + geom_histogram()
ggplot(data = eda.data, aes(x = scale(log(Fare+1)))) + geom_histogram() # log transformation to see how the distribution changes
```
<p>We scaled Fare to log(fare+1) to make the distribution for symterical</p>


<p>We will see the realtionship between ***Fare*** and other categorical varibales</p>
```{r, echo=FALSE}
ggplot(data=gather(eda.data, key = "Variable", value = "Value", -c(Age,Fare, Name, Ticket, Cabin)), aes(Value, Fare)) +facet_wrap(~ Variable, scale = "free_x") +geom_boxplot()
```

<p>There doesnt seem to be some insighful interactions between Fare and any predictor varibales</p>
<p>We will try to categorize the fare, it may be useful in our model. We will call this varibale ***Fare.factor***</p>

```{r, echo=FALSE}
a<-as.numeric(quantile(eda.data$Fare, c(0.25,0.50,0.75)))
b<-ifelse(eda.data$Fare>=0 & eda.data$Fare<=a[1], "low.fare",
          ifelse(eda.data$Fare>a[1] & eda.data$Fare<=a[2], "medium.fare",
                 ifelse(eda.data$Fare>a[2] & eda.data$Fare<=a[3], "high.fare", "very.high.fare")))
eda.data$Fare.factor<-as.factor(b)

a<-as.numeric(quantile(combined.data$Fare, c(0.25,0.50,0.75), na.rm = TRUE))
b<-ifelse(combined.data$Fare>=0 & combined.data$Fare<=a[1], "low.fare",
          ifelse(combined.data$Fare>a[1] & combined.data$Fare<=a[2], "medium.fare",
                 ifelse(combined.data$Fare>a[2] & combined.data$Fare<=a[3], "high.fare", "very.high.fare")))
combined.data$Fare.factor<-as.factor(b)
```

------
<font size="+2"><li>EDA Analysis 4: Name</li></font>
<p>We will extract the tittle in people's name so we can use them in our model. We will call this new factor ***Name.abr***. From the barplot, we see that ***Mr*** are the ones that died</p>

```{r, echo=FALSE}
Name.abr<-str_extract(string = eda.data$Name ,pattern = "(Mr.|Miss.|Mrs.|Master.|Dr.|Rev.|Col.|Mlle.|Major.|Ms.|Sir.|Capt.|Dona.|Don.|Lady|Mme)")
a<-ifelse(Name.abr %in% c("Mr.") , "Mr",
       ifelse(Name.abr %in% c("Mrs" , "Miss." , "Ms.") , "Mrs/Miss/Ms" ,
              ifelse(Name.abr %in% c("Master.") , "Master", "Others")))
eda.data$Name.abr<-as.factor(a)


Name.abr<-str_extract(string = combined.data$Name ,pattern = "(Mr.|Miss.|Mrs.|Master.|Dr.|Rev.|Col.|Mlle.|Major.|Ms.|Sir.|Capt.|Dona.|Don.|Lady|Mme)")
a<-ifelse(Name.abr %in% c("Mr.") , "Mr",
       ifelse(Name.abr %in% c("Mrs" , "Miss." , "Ms.") , "Mrs/Miss/Ms" ,
              ifelse(Name.abr %in% c("Master.") , "Master", "Others")))
combined.data$Name.abr<-as.factor(a)
```


```{r, echo=FALSE}
barplot(table(eda.data$Name.abr , eda.data$Survived), beside = TRUE, legend.text = TRUE)
```


<p>We will extract the last names of each passenger and call it ***Last.name***</p>
```{r, echo=FALSE}
first.name<-gsub(",.*$", "", eda.data$Name)
eda.data$Last.name<-as.factor(first.name)

first.name<-gsub(",.*$", "", combined.data$Name)
combined.data$Last.name<-as.factor(first.name)
```

-----

<font size="+2"><li>EDA Analysis 5: Family Size</li></font>

<p>From the kaggle description, we know that only female passengers survived, so we will create a custome varibale for that. If they are a ***female and if their age is less than 21.5*** we will say they are ***female and the survived***</p>
```{r, echo=FALSE}
eda.data$female.survived<-as.factor(ifelse(eda.data$Age <21.5 & eda.data$Sex=="female" , "female.and.kid" , "female.and.not.kid"))

combined.data$female.survived<-as.factor(ifelse(combined.data$Age <21.5 & combined.data$Sex=="female" , "female.and.kid" , "female.and.not.kid"))
```


<p>We will create a varibale for ***Is alone*** for only 1 passesnger when including family</p>
```{r, echo=FALSE}
eda.data$Is.alone<-as.factor(ifelse(eda.data$Family.size.numeric ==1, "is.alone", "not.alone"))
combined.data$Is.alone<-as.factor(ifelse(combined.data$Family.size.numeric ==1, "is.alone", "not.alone"))
```

-----
<font size="+2"><li>EDA Analysis 6: Cabin</li></font>
<p>Since there are alot of missing values, we will just impute those missing values with ***unknown***</p>
```{r, echo=FALSE}
eda.data$Cabin.impute<-substring(eda.data$Cabin,1,1)
eda.data$Cabin.impute[which(is.na(eda.data$Cabin.impute))]<- "unknown"
eda.data$Cabin.impute<-as.factor(eda.data$Cabin.impute)

combined.data$Cabin.impute<-substring(combined.data$Cabin,1,1)
combined.data$Cabin.impute[which(is.na(combined.data$Cabin.impute))]<- "unknown"
combined.data$Cabin.impute<-as.factor(combined.data$Cabin.impute)
```

----
<p>We will remove name out of our analysis since we don't need it anymore</p>
```{r, echo=FALSE}
eda.data<-eda.data %>% dplyr::select(-Name)
combined.data<-combined.data %>% dplyr::select(-Name)
```

-----
<font size="+3"><li>Feature Selection</li></font>
<p>We will use ***information gain*** as our source of varibale importance. This chart tells us what varibales will be very useful in our model. From the plot, we see that Ticket has the highest information, so we will still comtinue our analysis to see how we can use ***Ticket and Last.name***. Since these 2 varibales have alot of factors, we need another way to categorize them</p>
```{r, echo=FALSE}
eda.data.1<-eda.data %>% mutate_at(c("Survived", "Pclass", "Sex", "Embarked", "Ticket", "Cabin"), as.factor)

my.method<-"FSelectorRcpp_information.gain"
train.task.feature<-makeClassifTask(data = eda.data.1 , target = "Survived")
feature.importance<-generateFilterValuesData(train.task.feature, method = my.method)
plotFilterValues(feature.importance)

feature.importance$data 
```


<font size="+3"><li>Modelling</li></font>
<p>We will split the data into train and test</p>
```{r}
train.data<-filter(combined.data[train.kaggle$PassengerId,])
test.data<-combined.data[-train.kaggle$PassengerId,]
head(train.data, n=1);head(test.data, n=1)
```

<p>We will create a ***xgboost model*** and fine tune and parameters to get a ***pseduo survival*** score</p>


```{r, echo=FALSE}
train.data.xgb<-train.data %>% dplyr::select(c(Name.abr, Sex, Survived, Pclass))
train.data.xgb <-mutate_at(train.data.xgb, .vars = vars(-Survived), .funs = as.numeric)
xgb <- makeLearner("classif.xgboost")
train.data.xgb.task<-makeClassifTask(data = train.data.xgb, target = "Survived")

xgbParamSpace <- makeParamSet(
  makeNumericParam("eta", lower = 0, upper = 1),
  makeNumericParam("gamma", lower = 0, upper = 5),
  makeIntegerParam("max_depth", lower = 1, upper = 5),
  makeNumericParam("min_child_weight", lower = 1, upper = 10),
  makeNumericParam("subsample", lower = 0.5, upper = 1),
  makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
  makeIntegerParam("nrounds", lower = 50, upper = 50),
  makeDiscreteParam("eval_metric", values = c("error")))

randSearch <- makeTuneControlRandom(maxit = 100)

cvForTuning <- makeResampleDesc("Holdout", split = 0.75)

tunedXgbPars <- tuneParams(xgb, task = train.data.xgb.task, resampling = cvForTuning,
                           par.set = xgbParamSpace,control = randSearch)
```

```{r, echo=FALSE}
#tunedXgbPars

# This is our pseduo-survived values
# We will use these values for further feature engineering
train.data.xgb<-train.data %>% dplyr::select(c(Name.abr, Sex, Survived, Pclass))
train.data.xgb <-mutate_at(train.data.xgb, .vars = vars(-Survived), .funs = as.numeric)
xgb <- makeLearner("classif.xgboost")
train.data.xgb.task<-makeClassifTask(data = train.data.xgb, target = "Survived")

test.data.xgb<-test.data %>% dplyr::select(c(Name.abr, Sex, Survived, Pclass))
test.data.xgb <-mutate_at(test.data.xgb, .vars = vars(-Survived), .funs = as.numeric)

tunedXgb <- setHyperPars(xgb, par.vals = list(eta=0.0345, gamma=0.2, max_depth=4, min_child_weight=2, subsample=0.9, colsample_bytree=0.9, nrounds=100, eval_metric="error"))
tunedXgbModel <- train(tunedXgb, train.data.xgb.task)

a<-predict(tunedXgbModel, newdata = test.data.xgb)$data$response
#mean(real.values == a) 
```

<p>Our ***psudo survival*** has an accuracy of 0.78. We will treat this as a new factor and incorporate it into our analysis (dataframe)</p>


```{r}
test.data$xgb.survived<-a
train.data$xgb.survived<-train.data$Survived
combined.data<-mutate(bind_rows(train.data , test.data))
```

<p>We will now see if there is a relationship between ***Last name and our presudo survived***. We will group the values based on last name and if the mean of them is greater than 0.5, we wil call it 1, else 0 and we will call it ***Last.name.survived***</p>

```{r, echo=FALSE}
combined.data$xgb.survived<-as.numeric(as.character(combined.data$xgb.survived))
aa<-combined.data %>% group_by(Last.name) %>% summarise(Last.name.survived = 
                                                      ifelse(mean(xgb.survived)>0.5,1,0))

combined.data<-full_join(combined.data , aa)
head(combined.data, n=3)
```


```{r}
combined.data$Ticket.factor<-as.factor(substring(combined.data$Ticket,1,1))

aa<-combined.data %>% group_by(Ticket.factor) %>% summarise(Last.name.survived.ticket
                                                        =ifelse(mean(xgb.survived)>0.5,1,0))

combined.data<-full_join(combined.data , aa)
head(combined.data, n=3)
```




<p>We will resplit our data to see if the new features we created are more useful on our final model</p>

```{r}
combined.data$Last.name.survived<-as.factor(combined.data$Last.name.survived)
combined.data$Ticket<-as.factor(combined.data$Ticket)
combined.data$Last.name.survived.ticket<-as.factor(combined.data$Last.name.survived.ticket)
train.data<-filter(combined.data[train.kaggle$PassengerId,])
test.data<-combined.data[-train.kaggle$PassengerId,]
```


<p>The new varibale we have created ***Last.name.survived*** is an important feature</p>

```{r, echo=FALSE}
my.method<-"FSelectorRcpp_information.gain"
train.task.feature<-makeClassifTask(data = train.data , target = "Survived")
feature.importance<-generateFilterValuesData(train.task.feature, method = my.method)
plotFilterValues(feature.importance)
```
<p>We will create a sample ***Random forest model*** just to see how well it does with the real values. Note we did not touch the test data at all!</p>


<p>We will not fine tune our ***Random Forest*** to see if that increases accuracy</p>

```{r, echo=FALSE}
getParamSet(makeLearner("classif.randomForest"))
```

```{r, echo=FALSE}
train.rf<-train.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Survived, Fare))
test.rf<-test.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Fare))

train.data.task.rf<-makeClassifTask(data = train.rf , target = "Survived")

forest <- makeLearner("classif.randomForest")
forestParamSpace <- makeParamSet(
  makeIntegerParam("ntree", lower = 1000, upper = 1000),
  makeIntegerParam("mtry", lower = 1, upper = 5),
  makeIntegerParam("nodesize", lower = 1, upper = 20),
  makeIntegerParam("maxnodes", lower = 10, upper = 30))

randSearch <- makeTuneControlRandom(maxit = 200)

cvForTuning <- makeResampleDesc("Holdout", split = 0.80)

parallelStartSocket(cpus = detectCores())
tunedForestPars <- tuneParams(forest, task = train.data.task.rf, resampling = cvForTuning,
                              par.set = forestParamSpace, control = randSearch) 
parallelStop()

tunedForestPars 
```

```{r}
forest <- makeLearner("classif.randomForest")
train.data.task.rf<-makeClassifTask(data = train.rf , target = "Survived")
tunedForest <- setHyperPars(forest, par.vals = list(ntree=1000, mtry=2, nodesize=5, maxnodes=25))
tunedForestModel <- train(tunedForest, train.data.task.rf)
rf.final.predict.train.data<-predict(tunedForestModel , newdata = train.rf)$data$response
rf.final.predict.test.data<-predict(tunedForestModel, newdata = test.rf)$data$response
#mean(real.values==predict(tunedForestModel, newdata = test.rf)$data$response)

# 0.7942584
```


<p>We only did 100 searches to make it more generalizable. We see that tuning did help us.</p>

--------
<p>We will create a ***XGBOOST*** to see how the model performs</p>
```{r, echo=FALSE}
train.data.xgb<-train.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Survived, Fare))
train.data.xgb <-mutate_at(train.data.xgb, .vars = vars(-Survived), .funs = as.numeric)
xgb <- makeLearner("classif.xgboost")
train.data.xgb.task<-makeClassifTask(data = train.data.xgb, target = "Survived")

xgbParamSpace <- makeParamSet(
  makeNumericParam("eta", lower = 0, upper = 1),
  makeNumericParam("gamma", lower = 0, upper = 5),
  makeIntegerParam("max_depth", lower = 1, upper = 5),
  makeNumericParam("min_child_weight", lower = 1, upper = 10),
  makeNumericParam("subsample", lower = 0.5, upper = 1),
  makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
  makeIntegerParam("nrounds", lower = 50, upper = 50),
  makeDiscreteParam("eval_metric", values = c("error")))

randSearch <- makeTuneControlRandom(maxit = 100)

cvForTuning <- makeResampleDesc("Holdout", split = 0.75)

tunedXgbPars <- tuneParams(xgb, task = train.data.xgb.task, resampling = cvForTuning,
                           par.set = xgbParamSpace,control = randSearch)

tunedXgbPars

tunedXgb <- setHyperPars(xgb, par.vals = tunedXgbPars$x)
tunedXgbModel <- train(tunedXgb, train.data.xgb.task)

xgb.final.predict.train.data<- predict(tunedXgbModel, newdata =train.data.xgb)$data$response

test.xgb<-test.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Fare))
test.data.xgb <-mutate_all(test.xgb, .funs = as.numeric)

xgb.final.predict.test.data<-predict(tunedXgbModel, newdata = test.data.xgb)$data$response

#mean(real.values==predict(tunedXgbModel, newdata = test.data.xgb)$data$response)
```

```{r}
xgb <- makeLearner("classif.xgboost")

train.data.xgb<-train.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Survived, Fare))
train.data.xgb <-mutate_at(train.data.xgb, .vars = vars(-Survived), .funs = as.numeric)
train.data.xgb.task<-makeClassifTask(data = train.data.xgb, target = "Survived")

tunedXgb <- setHyperPars(xgb, par.vals = list(eta=0.3, gamma=2.64, max_depth=3, min_child_weight=2.75, subsample=0.7, colsample_bytree=0.9, nrounds=50, eval_metric = "error", nthread=1))
tunedXgbModel <- train(tunedXgb, train.data.xgb.task)

xgb.final.predict.train.data<- predict(tunedXgbModel, newdata =train.data.xgb)$data$response

test.xgb<-test.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Fare))
test.data.xgb <-mutate_all(test.xgb, .funs = as.numeric)

xgb.final.predict.test.data<-predict(tunedXgbModel, newdata = test.data.xgb)$data$response

#mean(real.values==predict(tunedXgbModel, newdata = test.data.xgb)$data$response)

# 0.7942584
```




<p>XGBOOST did really well and imporved the model slighly more.</p>


<p>We will create 1 final model. The final model will be ***ADABOOST***</p>

```{r}
getParamSet(makeLearner("classif.ada"))
```

```{r}
train.data.ada<-train.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Survived, Fare))
train.data.ada <-mutate_at(train.data.xgb, .vars = vars(-Survived), .funs = as.numeric)

ada <- makeLearner("classif.ada")
train.data.ada.task<-makeClassifTask(data = train.data.ada, target = "Survived")

adaParamSpace <- makeParamSet(
  makeNumericParam("nu", lower = 0.1, upper = 1),
  makeNumericParam("bag.frac", lower = 0, upper = 1),
  makeIntegerParam("iter", lower = 50, upper = 50),
  makeIntegerParam("max.iter", lower = 30, upper = 30))

randSearch <- makeTuneControlRandom(maxit = 100)

cvForTuning <- makeResampleDesc("Holdout", split = 0.75)

tunedadaPars <- tuneParams(ada, task = train.data.ada.task, resampling = cvForTuning,
                           par.set = adaParamSpace,control = randSearch)

tunedadaPars
```

```{r}
ada <- makeLearner("classif.ada")

train.data.ada<-train.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Survived, Fare))
train.data.ada <-mutate_at(train.data.xgb, .vars = vars(-Survived), .funs = as.numeric)
train.data.ada.task<-makeClassifTask(data = train.data.ada, target = "Survived")

tunedada <- setHyperPars(ada, par.vals = list(nu=0.1, bag.frac=0.9, iter=50, max.iter=30))
tunedadaModel <- train(tunedada, train.data.ada.task)
ada.final.predict.train.data<- predict(tunedadaModel, newdata =train.data.ada)$data$response
test.ada<-test.data %>% dplyr::select(c(Last.name.survived, Sex, Name.abr, Pclass, Fare))
test.data.ada <-mutate_all(test.xgb, .funs = as.numeric)
ada.final.predict.test.data<-predict(tunedadaModel, newdata = test.data.ada)$data$response

mean(real.values==predict(tunedadaModel, newdata = test.data.ada)$data$response)

# 0.7894737
```




<font size="+3"><li>Stacking</li></font>
```{r}
length(rf.final.predict.train.data)
length(xgb.final.predict.train.data)
length(ada.final.predict.train.data)
nrow(train.data)

stacking.train<- data.frame(rf = as.factor(rf.final.predict.train.data),
                            xgb = as.factor(xgb.final.predict.train.data),
                            ada = as.factor(ada.final.predict.train.data),
                            y = as.factor(as.factor(train.data$Survived)))

stacking.test<- data.frame(rf = as.factor(rf.final.predict.test.data),
                           xgb = as.factor(xgb.final.predict.test.data),
                           ada = as.factor(ada.final.predict.test.data))
```

```{r}
logit.stacking.train<-glm(y ~., data =stacking.train , family = binomial(link = "logit") )
summary(logit.stacking.train)

mean(ifelse(predict(logit.stacking.train , newdata = stacking.train, type = "response")>0.5,1,0) == stacking.train$y)

mean(ifelse(predict(logit.stacking.train, newdata = stacking.test , type = "response")>0.5,1,0)==real.values)
```


```{r}
final.pred<-rep(0, nrow(test.data))
final.pred[which(predict(logit.stacking.train, newdata = stacking.test , type = "response")>0.5)]<-1
dput(final.pred)
```


